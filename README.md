# MNN
# Create for Homework2 Mashine Learning NaUkma,by Denys Baranov

#The k-nearest neighbors method
//from wiki
(eng. K-nearest neighbors algorithm, k-NN) is a metric algorithm for automatic classification of objects or regression.
This is the most common form among {\ displaystyle k} k neighbors of a given element, whose classes are already known.
In the case of using a parameter for regression, the average value over {\ displaystyle k} k is the nearest object whose values are already known.

In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample. In some fields such as signal processing and econometrics it is also termed the Parzenâ€“Rosenblatt window method, after Emanuel Parzen and Murray Rosenblatt, who are usually credited with independently creating it in its current form.[1][2]


The method of standards is finding the most vivid representative of a class and marking new subjects in relation to such standards.

![window program](https://i.paste.pics/1235581b19439dabc286e8d7142ea32f.png)
